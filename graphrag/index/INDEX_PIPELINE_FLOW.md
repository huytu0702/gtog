# GraphRAG Indexing Pipeline - Flow Documentation

## ğŸ“‹ Tá»•ng Quan

Module `graphrag/index` chá»‹u trÃ¡ch nhiá»‡m xÃ¢y dá»±ng **Knowledge Graph** tá»« dá»¯ liá»‡u vÄƒn báº£n thÃ´. Pipeline nÃ y biáº¿n Ä‘á»•i documents thÃ nh má»™t Ä‘á»“ thá»‹ tri thá»©c cÃ³ cáº¥u trÃºc, bao gá»“m entities, relationships, communities vÃ  community reports.

---

## ğŸ—‚ï¸ Cáº¥u TrÃºc ThÆ° Má»¥c

```
graphrag/index/
â”œâ”€â”€ __init__.py              # Package root
â”œâ”€â”€ validate_config.py       # Validation cáº¥u hÃ¬nh
â”œâ”€â”€ input/                   # Input loaders (CSV, JSON, Text)
â”œâ”€â”€ operations/              # CÃ¡c thao tÃ¡c xá»­ lÃ½ dá»¯ liá»‡u
â”œâ”€â”€ run/                     # Pipeline execution
â”œâ”€â”€ text_splitting/          # Text chunking utilities
â”œâ”€â”€ typing/                  # Type definitions
â”œâ”€â”€ update/                  # Incremental update logic
â”œâ”€â”€ utils/                   # Utility functions
â””â”€â”€ workflows/               # Workflow definitions
```

---

## ğŸ”„ Luá»“ng Indexing - Standard Pipeline

### Mermaid Diagram - Tá»•ng Quan Luá»“ng

```mermaid
flowchart TB
    subgraph "1. INPUT PHASE"
        A[ğŸ“„ Raw Documents<br/>CSV/JSON/TXT] --> B[load_input_documents]
        B --> C[(documents.parquet)]
    end

    subgraph "2. TEXT PROCESSING PHASE"
        C --> D[create_base_text_units]
        D --> E[(text_units.parquet)]
        E --> F[create_final_documents]
        F --> G[(documents.parquet<br/>with text_unit_ids)]
    end

    subgraph "3. GRAPH EXTRACTION PHASE"
        E --> H[extract_graph]
        H --> I{LLM Entity<br/>Extraction}
        I --> J[(entities.parquet)]
        I --> K[(relationships.parquet)]
        
        J --> L[finalize_graph]
        K --> L
        L --> M[(entities.parquet<br/>finalized)]
        L --> N[(relationships.parquet<br/>finalized)]
    end

    subgraph "4. COVARIATES PHASE"
        E --> O[extract_covariates]
        O --> P{LLM Claim<br/>Extraction}
        P --> Q[(covariates.parquet)]
    end

    subgraph "5. COMMUNITY DETECTION PHASE"
        M --> R[create_communities]
        N --> R
        R --> S{Hierarchical<br/>Leiden Algorithm}
        S --> T[(communities.parquet)]
    end

    subgraph "6. TEXT UNITS FINALIZATION"
        E --> U[create_final_text_units]
        M --> U
        N --> U
        Q --> U
        U --> V[(text_units.parquet<br/>finalized)]
    end

    subgraph "7. COMMUNITY REPORTS PHASE"
        M --> W[create_community_reports]
        N --> W
        T --> W
        Q --> W
        W --> X{LLM Summary<br/>Generation}
        X --> Y[(community_reports.parquet)]
    end

    subgraph "8. EMBEDDING PHASE"
        G --> Z[generate_text_embeddings]
        V --> Z
        M --> Z
        N --> Z
        Y --> Z
        Z --> AA{Embedding<br/>Model}
        AA --> AB[(Vector Embeddings<br/>for Search)]
    end

    style A fill:#e1f5fe
    style AB fill:#c8e6c9
    style I fill:#fff3e0
    style P fill:#fff3e0
    style S fill:#f3e5f5
    style X fill:#fff3e0
    style AA fill:#fce4ec
```

---

## ğŸ“Š Chi Tiáº¿t Tá»«ng Workflow

### Mermaid Diagram - Pipeline Workflows

```mermaid
graph LR
    subgraph "Standard Pipeline"
        W1[load_input_documents] --> W2[create_base_text_units]
        W2 --> W3[create_final_documents]
        W3 --> W4[extract_graph]
        W4 --> W5[finalize_graph]
        W5 --> W6[extract_covariates]
        W6 --> W7[create_communities]
        W7 --> W8[create_final_text_units]
        W8 --> W9[create_community_reports]
        W9 --> W10[generate_text_embeddings]
    end

    style W1 fill:#bbdefb
    style W4 fill:#ffe0b2
    style W6 fill:#ffe0b2
    style W7 fill:#e1bee7
    style W9 fill:#ffe0b2
    style W10 fill:#f8bbd9
```

---

## ğŸ” MÃ´ Táº£ Chi Tiáº¿t Tá»«ng BÆ°á»›c

### 1ï¸âƒ£ Load Input Documents

**File:** `workflows/load_input_documents.py`

**Má»¥c Ä‘Ã­ch:** Äá»c vÃ  parse dá»¯ liá»‡u Ä‘áº§u vÃ o tá»« cÃ¡c nguá»“n khÃ¡c nhau.

**Input Factory há»— trá»£:**
- `text` - Plain text files (`.txt`)
- `csv` - CSV files with text columns
- `json` - JSON files

```mermaid
flowchart LR
    subgraph "Input Sources"
        TXT[ğŸ“„ .txt files]
        CSV[ğŸ“Š .csv files]
        JSON[ğŸ“‹ .json files]
    end
    
    TXT --> Factory[Input Factory]
    CSV --> Factory
    JSON --> Factory
    
    Factory --> DF["pd.DataFrame<br/>[id, text, title, metadata]"]
    DF --> Storage[(documents.parquet)]
```

**VÃ­ dá»¥ cáº¥u trÃºc output:**

```python
# documents DataFrame
{
    "id": "doc_001",
    "text": "Ná»™i dung vÄƒn báº£n Ä‘áº§y Ä‘á»§ cá»§a tÃ i liá»‡u...",
    "title": "TÃªn tÃ i liá»‡u",
    "metadata": {"author": "John Doe", "date": "2024-01-01"}
}
```

---

### 2ï¸âƒ£ Create Base Text Units

**File:** `workflows/create_base_text_units.py`

**Má»¥c Ä‘Ã­ch:** Chia nhá» documents thÃ nh cÃ¡c text chunks (text units) Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£ hÆ¡n.

```mermaid
flowchart TB
    subgraph "Chunking Process"
        DOC[ğŸ“„ Long Document<br/>~10,000 tokens] --> CHUNK[Text Chunking]
        CHUNK --> U1[Chunk 1<br/>~1200 tokens]
        CHUNK --> U2[Chunk 2<br/>~1200 tokens]
        CHUNK --> U3[Chunk 3<br/>~1200 tokens]
        CHUNK --> UN[Chunk N<br/>...]
    end
    
    subgraph "Chunking Strategies"
        S1[tokens - Token-based splitting]
        S2[sentence - Sentence-based splitting]
    end
    
    CHUNK -.-> S1
    CHUNK -.-> S2
```

**Cáº¥u hÃ¬nh chunking:**

```yaml
chunks:
  size: 1200        # Sá»‘ tokens má»—i chunk
  overlap: 100      # Sá»‘ tokens overlap giá»¯a cÃ¡c chunks
  strategy: tokens  # tokens hoáº·c sentence
  encoding_model: cl100k_base
```

**VÃ­ dá»¥ output:**

```python
# text_units DataFrame
{
    "id": "tu_hash_001",
    "text": "ÄÃ¢y lÃ  ná»™i dung cá»§a text unit Ä‘áº§u tiÃªn...",
    "document_ids": ["doc_001"],
    "n_tokens": 1150
}
```

---

### 3ï¸âƒ£ Create Final Documents

**File:** `workflows/create_final_documents.py`

**Má»¥c Ä‘Ã­ch:** Cáº­p nháº­t documents vá»›i danh sÃ¡ch text_unit_ids liÃªn quan.

```mermaid
flowchart LR
    D1[Documents] --> JOIN[Join Operation]
    TU[Text Units] --> JOIN
    JOIN --> FD[Final Documents<br/>with text_unit_ids]
```

**VÃ­ dá»¥ output:**

```python
# documents DataFrame (updated)
{
    "id": "doc_001",
    "title": "TÃªn tÃ i liá»‡u",
    "text": "Ná»™i dung Ä‘áº§y Ä‘á»§...",
    "text_unit_ids": ["tu_001", "tu_002", "tu_003"],
    "metadata": {...}
}
```

---

### 4ï¸âƒ£ Extract Graph (Core LLM Operation)

**File:** `workflows/extract_graph.py`

**Má»¥c Ä‘Ã­ch:** Sá»­ dá»¥ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t entities vÃ  relationships tá»« text.

```mermaid
flowchart TB
    subgraph "Entity & Relationship Extraction"
        TU[Text Unit] --> LLM[ğŸ¤– LLM<br/>GPT-4/Claude/etc]
        
        LLM --> ENT["Entities<br/>(Person, Organization, Location, Event)"]
        LLM --> REL["Relationships<br/>(WORKS_FOR, LOCATED_IN, etc)"]
    end
    
    subgraph "Summarization"
        ENT --> SUM1[Description Summarization]
        REL --> SUM2[Description Summarization]
        
        SUM1 --> FENT[Final Entities<br/>with merged descriptions]
        SUM2 --> FREL[Final Relationships<br/>with merged descriptions]
    end
    
    subgraph "Entity Merging"
        E1["Entity: 'Microsoft'<br/>from TU1"]
        E2["Entity: 'Microsoft'<br/>from TU2"]
        E3["Entity: 'Microsoft'<br/>from TU3"]
        
        E1 --> MERGE[Merge by Title]
        E2 --> MERGE
        E3 --> MERGE
        
        MERGE --> EM["Merged Entity<br/>descriptions: [d1, d2, d3]<br/>text_unit_ids: [tu1, tu2, tu3]<br/>frequency: 3"]
    end
```

**Entity Types máº·c Ä‘á»‹nh:**
- `organization` - Tá»• chá»©c, cÃ´ng ty
- `person` - NgÆ°á»i
- `geo` - Äá»‹a Ä‘iá»ƒm Ä‘á»‹a lÃ½
- `event` - Sá»± kiá»‡n

**VÃ­ dá»¥ extraction:**

```python
# Input Text Unit
text = """
Microsoft, cÃ´ng ty cÃ´ng nghá»‡ cÃ³ trá»¥ sá»Ÿ táº¡i Redmond, Washington, 
Ä‘Æ°á»£c thÃ nh láº­p bá»Ÿi Bill Gates vÃ  Paul Allen vÃ o nÄƒm 1975.
"""

# Extracted Entities
entities = [
    {"title": "MICROSOFT", "type": "organization", 
     "description": "CÃ´ng ty cÃ´ng nghá»‡ cÃ³ trá»¥ sá»Ÿ táº¡i Redmond"},
    {"title": "BILL GATES", "type": "person", 
     "description": "Äá»“ng sÃ¡ng láº­p Microsoft"},
    {"title": "PAUL ALLEN", "type": "person", 
     "description": "Äá»“ng sÃ¡ng láº­p Microsoft"},
    {"title": "REDMOND", "type": "geo", 
     "description": "ThÃ nh phá»‘ táº¡i Washington, trá»¥ sá»Ÿ Microsoft"}
]

# Extracted Relationships
relationships = [
    {"source": "MICROSOFT", "target": "REDMOND", 
     "description": "cÃ³ trá»¥ sá»Ÿ táº¡i", "weight": 1.0},
    {"source": "BILL GATES", "target": "MICROSOFT", 
     "description": "Ä‘á»“ng sÃ¡ng láº­p", "weight": 1.0},
    {"source": "PAUL ALLEN", "target": "MICROSOFT", 
     "description": "Ä‘á»“ng sÃ¡ng láº­p", "weight": 1.0}
]
```

---

### 5ï¸âƒ£ Finalize Graph

**File:** `workflows/finalize_graph.py`

**Má»¥c Ä‘Ã­ch:** HoÃ n thiá»‡n format cá»§a entities vÃ  relationships, tÃ­nh toÃ¡n cÃ¡c metrics.

```mermaid
flowchart TB
    subgraph "Finalization Steps"
        E[Entities] --> FE[finalize_entities]
        R[Relationships] --> FR[finalize_relationships]
        
        FE --> |Add metrics| EF[Final Entities]
        FR --> |Add metrics| RF[Final Relationships]
        
        subgraph "Added Fields"
            M1[human_readable_id]
            M2[node_degree]
            M3[edge_degree]
            M4[combined_degree]
        end
    end
    
    EF --> GRAPHML[Optional: GraphML Snapshot]
    RF --> GRAPHML
```

**VÃ­ dá»¥ output:**

```python
# entities DataFrame (finalized)
{
    "id": "ent_uuid_001",
    "title": "MICROSOFT",
    "type": "organization",
    "description": "Summarized description...",
    "human_readable_id": 0,
    "text_unit_ids": ["tu_001", "tu_002"],
    "frequency": 5,
    "degree": 12  # Sá»‘ connections trong graph
}

# relationships DataFrame (finalized)
{
    "id": "rel_uuid_001",
    "source": "MICROSOFT",
    "target": "REDMOND",
    "description": "cÃ³ trá»¥ sá»Ÿ táº¡i",
    "weight": 3.0,
    "human_readable_id": 0,
    "source_degree": 12,
    "target_degree": 5,
    "combined_degree": 17
}
```

---

### 6ï¸âƒ£ Extract Covariates (Optional)

**File:** `workflows/extract_covariates.py`

**Má»¥c Ä‘Ã­ch:** TrÃ­ch xuáº¥t claims/covariates tá»« text (náº¿u Ä‘Æ°á»£c enable).

```mermaid
flowchart LR
    TU[Text Units] --> LLM[ğŸ¤– LLM Claim Extraction]
    LLM --> COV["Covariates/Claims<br/>(subject, type, status, description)"]
    COV --> Storage[(covariates.parquet)]
```

**VÃ­ dá»¥ claim:**

```python
{
    "id": "claim_001",
    "subject": "MICROSOFT",
    "type": "FINANCIAL_CLAIM",
    "status": "TRUE",
    "description": "Microsoft cÃ³ doanh thu hÃ ng nÄƒm trÃªn 100 tá»· USD",
    "text_unit_id": "tu_005"
}
```

---

### 7ï¸âƒ£ Create Communities

**File:** `workflows/create_communities.py`

**Má»¥c Ä‘Ã­ch:** PhÃ¡t hiá»‡n cá»™ng Ä‘á»“ng trong graph sá»­ dá»¥ng thuáº­t toÃ¡n Hierarchical Leiden.

```mermaid
flowchart TB
    subgraph "Community Detection"
        G[Graph<br/>Entities + Relationships] --> LEIDEN[Hierarchical Leiden<br/>Algorithm]
        
        LEIDEN --> L0[Level 0 Communities<br/>Fine-grained]
        LEIDEN --> L1[Level 1 Communities<br/>Medium-grained]
        LEIDEN --> L2[Level 2 Communities<br/>Coarse-grained]
        
        subgraph "Hierarchy Example"
            C0A[Community 0-A<br/>5 entities]
            C0B[Community 0-B<br/>3 entities]
            C0C[Community 0-C<br/>4 entities]
            
            C1A[Community 1-A<br/>8 entities]
            C1B[Community 1-B<br/>4 entities]
            
            C2A[Community 2-A<br/>12 entities]
            
            C0A --> C1A
            C0B --> C1A
            C0C --> C1B
            C1A --> C2A
            C1B --> C2A
        end
    end
```

**Cáº¥u hÃ¬nh:**

```yaml
cluster_graph:
  max_cluster_size: 10  # KÃ­ch thÆ°á»›c cluster tá»‘i Ä‘a
  use_lcc: true         # Sá»­ dá»¥ng Largest Connected Component
  seed: 0xDEADBEEF      # Random seed for reproducibility
```

**VÃ­ dá»¥ output:**

```python
# communities DataFrame
{
    "id": "comm_uuid_001",
    "community": 0,
    "level": 0,
    "title": "Community 0",
    "parent": -1,
    "children": [1, 2, 3],
    "entity_ids": ["ent_001", "ent_002", "ent_003"],
    "relationship_ids": ["rel_001", "rel_002"],
    "text_unit_ids": ["tu_001", "tu_002"],
    "size": 3
}
```

---

### 8ï¸âƒ£ Create Final Text Units

**File:** `workflows/create_final_text_units.py`

**Má»¥c Ä‘Ã­ch:** Cáº­p nháº­t text units vá»›i references Ä‘áº¿n entities, relationships, vÃ  covariates.

```mermaid
flowchart LR
    TU[Text Units] --> JOIN1[+ entity_ids]
    E[Entities] --> JOIN1
    
    JOIN1 --> JOIN2[+ relationship_ids]
    R[Relationships] --> JOIN2
    
    JOIN2 --> JOIN3[+ covariate_ids]
    COV[Covariates] --> JOIN3
    
    JOIN3 --> FTU[Final Text Units]
```

**VÃ­ dá»¥ output:**

```python
# text_units DataFrame (finalized)
{
    "id": "tu_001",
    "text": "Ná»™i dung text unit...",
    "document_ids": ["doc_001"],
    "n_tokens": 1150,
    "entity_ids": ["ent_001", "ent_002"],
    "relationship_ids": ["rel_001"],
    "covariate_ids": ["claim_001"]
}
```

---

### 9ï¸âƒ£ Create Community Reports

**File:** `workflows/create_community_reports.py`

**Má»¥c Ä‘Ã­ch:** Sá»­ dá»¥ng LLM Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o tá»•ng há»£p cho má»—i community.

```mermaid
flowchart TB
    subgraph "Context Building"
        E[Entities in Community]
        R[Relationships in Community]
        C[Claims in Community]
        
        E --> CONTEXT[Build Local Context]
        R --> CONTEXT
        C --> CONTEXT
    end
    
    subgraph "Report Generation"
        CONTEXT --> LLM[ğŸ¤– LLM Summary]
        
        LLM --> REPORT["Community Report<br/>- Title<br/>- Summary<br/>- Full Content<br/>- Rating<br/>- Findings"]
    end
    
    subgraph "Hierarchical Processing"
        L0[Level 0 Communities] --> PROC[Process by Level]
        L1[Level 1 Communities] --> PROC
        L2[Level 2 Communities] --> PROC
        
        PROC --> |Sub-report inheritance| FINAL[Final Reports]
    end
```

**VÃ­ dá»¥ output:**

```python
# community_reports DataFrame
{
    "id": "report_001",
    "community": 0,
    "level": 0,
    "title": "Microsoft Technology Ecosystem",
    "summary": "Cá»™ng Ä‘á»“ng nÃ y táº­p trung vÃ o Microsoft vÃ  cÃ¡c má»‘i quan há»‡...",
    "full_content": "# Microsoft Technology Ecosystem\n\n## Overview\n...",
    "rating": 8.5,
    "rating_explanation": "High impact due to major tech company involvement",
    "findings": [
        "Microsoft lÃ  cÃ´ng ty cÃ´ng nghá»‡ lá»›n cÃ³ trá»¥ sá»Ÿ táº¡i Redmond",
        "Bill Gates vÃ  Paul Allen lÃ  nhá»¯ng ngÆ°á»i sÃ¡ng láº­p",
        ...
    ]
}
```

---

### ğŸ”Ÿ Generate Text Embeddings

**File:** `workflows/generate_text_embeddings.py`

**Má»¥c Ä‘Ã­ch:** Táº¡o vector embeddings cho tÃ¬m kiáº¿m semantic.

```mermaid
flowchart TB
    subgraph "Embedding Targets"
        D[ğŸ“„ Documents - text]
        TU[ğŸ“ Text Units - text]
        E[ğŸ‘¤ Entities - title, description]
        R[ğŸ”— Relationships - description]
        CR[ğŸ“Š Community Reports - title, summary, full_content]
    end
    
    subgraph "Embedding Process"
        D --> EMB[Embedding Model<br/>OpenAI/Local]
        TU --> EMB
        E --> EMB
        R --> EMB
        CR --> EMB
        
        EMB --> VEC[Vector Store<br/>LanceDB/FAISS/etc]
    end
    
    subgraph "Embedding Types"
        ET[entity_title_embedding]
        ED[entity_description_embedding]
        TUT[text_unit_text_embedding]
        CT[community_title_embedding]
        CS[community_summary_embedding]
        CF[community_full_content_embedding]
    end
```

**Cáº¥u hÃ¬nh embeddings:**

```yaml
embed_text:
  enabled: true
  model_id: text-embedding-ada-002
  names:
    - entity_description_embedding
    - text_unit_text_embedding
    - community_summary_embedding
```

---

## âš¡ Fast Pipeline

Fast pipeline sá»­ dá»¥ng NLP extraction thay vÃ¬ LLM, nhanh hÆ¡n nhÆ°ng kÃ©m chÃ­nh xÃ¡c hÆ¡n.

```mermaid
graph LR
    subgraph "Fast Pipeline"
        F1[load_input_documents] --> F2[create_base_text_units]
        F2 --> F3[create_final_documents]
        F3 --> F4[extract_graph_nlp]
        F4 --> F5[prune_graph]
        F5 --> F6[finalize_graph]
        F6 --> F7[create_communities]
        F7 --> F8[create_final_text_units]
        F8 --> F9[create_community_reports_text]
        F9 --> F10[generate_text_embeddings]
    end
    
    style F4 fill:#c8e6c9
    style F5 fill:#c8e6c9
    style F9 fill:#c8e6c9
```

**KhÃ¡c biá»‡t vá»›i Standard:**
- `extract_graph_nlp` - Sá»­ dá»¥ng NLP thay vÃ¬ LLM
- `prune_graph` - Loáº¡i bá» noise tá»« NLP extraction
- `create_community_reports_text` - Text-based reports thay vÃ¬ LLM

---

## ğŸ”„ Incremental Update Pipeline

Cho phÃ©p cáº­p nháº­t index mÃ  khÃ´ng cáº§n rebuild tá»« Ä‘áº§u.

```mermaid
graph LR
    subgraph "Update Pipeline"
        U1[load_update_documents] --> U2[Standard Workflows...]
        U2 --> U3[update_final_documents]
        U3 --> U4[update_entities_relationships]
        U4 --> U5[update_text_units]
        U5 --> U6[update_covariates]
        U6 --> U7[update_communities]
        U7 --> U8[update_community_reports]
        U8 --> U9[update_text_embeddings]
        U9 --> U10[update_clean_state]
    end
    
    style U3 fill:#fff3e0
    style U4 fill:#fff3e0
    style U5 fill:#fff3e0
    style U6 fill:#fff3e0
    style U7 fill:#fff3e0
    style U8 fill:#fff3e0
    style U9 fill:#fff3e0
```

---

## ğŸ“¤ Output Files

Sau khi hoÃ n thÃ nh indexing, cÃ¡c files sau Ä‘Æ°á»£c táº¡o:

```
output/
â”œâ”€â”€ documents.parquet          # TÃ i liá»‡u gá»‘c vá»›i metadata
â”œâ”€â”€ text_units.parquet         # Text chunks vá»›i references
â”œâ”€â”€ entities.parquet           # Entities Ä‘Æ°á»£c trÃ­ch xuáº¥t
â”œâ”€â”€ relationships.parquet      # Relationships giá»¯a entities
â”œâ”€â”€ communities.parquet        # Community assignments
â”œâ”€â”€ community_reports.parquet  # LLM-generated summaries
â”œâ”€â”€ covariates.parquet         # Claims/Covariates (náº¿u enabled)
â”œâ”€â”€ context.json               # Pipeline state
â””â”€â”€ stats.json                 # Execution statistics
```

---

## ğŸ› ï¸ VÃ­ Dá»¥ Cháº¡y Pipeline

### Command Line

```bash
# Standard indexing
graphrag index --root ./my-project

# Fast indexing (NLP-based)
graphrag index --root ./my-project --method fast

# Incremental update
graphrag index --root ./my-project --update

# With verbose logging
graphrag index --root ./my-project --verbose
```

### Python API

```python
import asyncio
from graphrag.index import run_pipeline
from graphrag.config import GraphRagConfig
from graphrag.index.workflows import PipelineFactory

async def main():
    config = GraphRagConfig.from_file("settings.yaml")
    
    # Create pipeline
    pipeline = PipelineFactory.create_pipeline(
        config, 
        method="standard"  # or "fast", "update"
    )
    
    # Run pipeline
    async for result in run_pipeline(pipeline, config, callbacks):
        print(f"Completed: {result.workflow}")

asyncio.run(main())
```

---

## ğŸ“Š Data Flow Summary

```mermaid
flowchart TB
    subgraph "Data Transformation"
        RAW["ğŸ“„ Raw Text<br/>Unstructured"] 
        --> CHUNKS["ğŸ“ Text Units<br/>Chunked"]
        --> GRAPH["ğŸ”· Knowledge Graph<br/>Entities + Relationships"]
        --> COMMUNITIES["ğŸ”¶ Communities<br/>Clustered"]
        --> REPORTS["ğŸ“Š Reports<br/>Summarized"]
        --> VECTORS["ğŸ§® Embeddings<br/>Searchable"]
    end
    
    style RAW fill:#ffcdd2
    style CHUNKS fill:#fff9c4
    style GRAPH fill:#c8e6c9
    style COMMUNITIES fill:#bbdefb
    style REPORTS fill:#e1bee7
    style VECTORS fill:#d1c4e9
```

---

## ğŸ”— LiÃªn Káº¿t Giá»¯a CÃ¡c ThÃ nh Pháº§n

```mermaid
erDiagram
    DOCUMENT ||--o{ TEXT_UNIT : contains
    TEXT_UNIT ||--o{ ENTITY : mentions
    TEXT_UNIT ||--o{ RELATIONSHIP : mentions
    TEXT_UNIT ||--o{ COVARIATE : has
    
    ENTITY ||--o{ RELATIONSHIP : source_or_target
    ENTITY }o--o{ COMMUNITY : belongs_to
    
    COMMUNITY ||--o| COMMUNITY_REPORT : has
    COMMUNITY ||--o{ COMMUNITY : parent_child
    
    ENTITY ||--o| EMBEDDING : has
    TEXT_UNIT ||--o| EMBEDDING : has
    COMMUNITY_REPORT ||--o| EMBEDDING : has
```

---

## ğŸ“ Káº¿t Luáº­n

GraphRAG Indexing Pipeline lÃ  má»™t há»‡ thá»‘ng phá»©c táº¡p nhÆ°ng Ä‘Æ°á»£c thiáº¿t káº¿ module hÃ³a cao. Má»—i workflow Ä‘áº£m nháº­n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¹y chá»‰nh hoáº·c thay tháº¿. Pipeline há»— trá»£ cáº£:

1. **Standard Mode** - Sá»­ dá»¥ng LLM cho extraction chÃ­nh xÃ¡c
2. **Fast Mode** - Sá»­ dá»¥ng NLP cho tá»‘c Ä‘á»™
3. **Update Mode** - Cáº­p nháº­t incremental hiá»‡u quáº£

Output cuá»‘i cÃ¹ng lÃ  má»™t Knowledge Graph hoÃ n chá»‰nh vá»›i embeddings, sáºµn sÃ ng cho cÃ¡c phÆ°Æ¡ng thá»©c query nhÆ° Local Search, Global Search, vÃ  ToG Search.
