# GraphRAG Document Indexing Flow

## Tá»•ng quan

Há»‡ thá»‘ng GraphRAG sá»­ dá»¥ng má»™t pipeline indexing phá»©c táº¡p Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c document thÃ´ thÃ nh má»™t knowledge graph cÃ³ cáº¥u trÃºc, Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho viá»‡c truy váº¥n vÃ  tÃ¬m kiáº¿m thÃ´ng tin. QuÃ¡ trÃ¬nh indexing diá»…n ra qua nhiá»u workflow tuáº§n tá»±, má»—i workflow thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ trong viá»‡c xá»­ lÃ½ vÃ  lÃ m giÃ u dá»¯ liá»‡u.

## Kiáº¿n trÃºc Pipeline

Há»‡ thá»‘ng há»— trá»£ 4 loáº¡i pipeline indexing:

1. **Standard Indexing** - Pipeline Ä‘áº§y Ä‘á»§ vá»›i LLM-based entity extraction
2. **Fast Indexing** - Pipeline nhanh hÆ¡n sá»­ dá»¥ng NLP-based extraction
3. **Standard Update** - Cáº­p nháº­t incremental cho pipeline standard
4. **Fast Update** - Cáº­p nháº­t incremental cho pipeline fast

## Flow Chart - Standard Indexing Pipeline

```mermaid
graph TB
    Start([Báº¯t Ä‘áº§u Indexing]) --> LoadDocs[1. Load Input Documents]
    
    LoadDocs --> ChunkText[2. Create Base Text Units]
    ChunkText --> ExtractGraph[3. Extract Graph]
    
    ExtractGraph --> FinalizeGraph[4. Finalize Graph]
    FinalizeGraph --> ExtractCov[5. Extract Covariates]
    
    ExtractCov --> CreateComm[6. Create Communities]
    CreateComm --> FinalTextUnits[7. Create Final Text Units]
    
    FinalTextUnits --> CreateReports[8. Create Community Reports]
    CreateReports --> GenEmbeddings[9. Generate Text Embeddings]
    
    GenEmbeddings --> End([HoÃ n thÃ nh])
    
    style Start fill:#e1f5e1
    style End fill:#e1f5e1
    style LoadDocs fill:#fff4e6
    style ChunkText fill:#fff4e6
    style ExtractGraph fill:#e3f2fd
    style FinalizeGraph fill:#e3f2fd
    style ExtractCov fill:#f3e5f5
    style CreateComm fill:#fce4ec
    style FinalTextUnits fill:#fce4ec
    style CreateReports fill:#fff9c4
    style GenEmbeddings fill:#e0f2f1
```

## Flow Chart - Fast Indexing Pipeline

```mermaid
graph TB
    Start([Báº¯t Ä‘áº§u Indexing]) --> LoadDocs[1. Load Input Documents]
    
    LoadDocs --> ChunkText[2. Create Base Text Units]
    ChunkText --> ExtractGraphNLP[3. Extract Graph NLP]
    
    ExtractGraphNLP --> PruneGraph[4. Prune Graph]
    PruneGraph --> FinalizeGraph[5. Finalize Graph]
    FinalizeGraph --> CreateComm[6. Create Communities]
    
    CreateComm --> FinalTextUnits[7. Create Final Text Units]
    FinalTextUnits --> GenEmbeddings[8. Generate Text Embeddings]
    
    GenEmbeddings --> End([HoÃ n thÃ nh])
    
    style Start fill:#e1f5e1
    style End fill:#e1f5e1
    style LoadDocs fill:#fff4e6
    style ChunkText fill:#fff4e6
    style ExtractGraphNLP fill:#e3f2fd
    style PruneGraph fill:#ffecb3
    style FinalizeGraph fill:#e3f2fd
    style CreateComm fill:#fce4ec
    style FinalTextUnits fill:#fce4ec
    style GenEmbeddings fill:#e0f2f1
```

## Chi tiáº¿t tá»«ng Workflow

### 1. Load Input Documents
**File**: `graphrag/index/workflows/load_input_documents.py`

**Má»¥c Ä‘Ã­ch**: Äá»c vÃ  phÃ¢n tÃ­ch cÃ¡c document Ä‘áº§u vÃ o tá»« storage (file system, blob storage, etc.)

**Quy trÃ¬nh**:
```mermaid
graph LR
    A[Input Storage] --> B[Create Input Handler]
    B --> C[Parse Documents]
    C --> D[Validate Format]
    D --> E[Output: documents.parquet]
    
    style A fill:#e8f5e9
    style E fill:#c8e6c9
```

**Input**: 
- Raw documents tá»« `input/` directory
- Supported formats: text, csv, json

**Output**: 
- DataFrame vá»›i schema:
  - `id`: Document identifier
  - `text`: Document content
  - `metadata`: Optional metadata (JSON string)

**VÃ­ dá»¥ thá»±c táº¿**:
```python
# Input: input/document1.txt
"""
CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020 bá»Ÿi Nguyá»…n VÄƒn A. 
CÃ´ng ty chuyÃªn vá» phÃ¡t triá»ƒn pháº§n má»m AI.
"""

# Output: documents.parquet
{
    "id": "doc_001",
    "text": "CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020...",
    "metadata": "{\"source\": \"document1.txt\", \"type\": \"text\"}"
}
```

---

### 2. Create Base Text Units
**File**: `graphrag/index/workflows/create_base_text_units.py`

**Má»¥c Ä‘Ã­ch**: Chia nhá» documents thÃ nh cÃ¡c text chunks cÃ³ kÃ­ch thÆ°á»›c phÃ¹ há»£p Ä‘á»ƒ xá»­ lÃ½

**Quy trÃ¬nh**:
```mermaid
graph TB
    A[Documents DataFrame] --> B[Group by Columns]
    B --> C{Chunking Strategy}
    C -->|Tokens| D[Token-based Chunking]
    C -->|Sentence| E[Sentence-based Chunking]
    D --> F[Apply Overlap]
    E --> F
    F --> G[Generate Chunk IDs]
    G --> H[Output: text_units.parquet]
    
    style A fill:#e8f5e9
    style H fill:#c8e6c9
```

**Thuáº­t toÃ¡n chunking**:
1. Group documents theo `group_by_columns` (default: `[id]`)
2. Aggregate text tá»« táº¥t cáº£ documents trong group
3. Split text thÃ nh chunks vá»›i:
   - `size`: Maximum tokens per chunk (default: 1200)
   - `overlap`: Tokens overlap giá»¯a cÃ¡c chunks (default: 100)
4. Prepend metadata náº¿u Ä‘Æ°á»£c config
5. Táº¡o unique SHA512 hash cho má»—i chunk

**Output**:
- DataFrame vá»›i schema:
  - `id`: Chunk identifier (SHA512 hash)
  - `text`: Chunk content
  - `document_ids`: List of source document IDs
  - `n_tokens`: Number of tokens in chunk

**VÃ­ dá»¥ thá»±c táº¿**:
```python
# Input: 1 document vá»›i 2500 tokens
# Config: size=1200, overlap=100

# Output: 3 text units
[
    {
        "id": "chunk_001_hash",
        "text": "CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p...",  # tokens 0-1200
        "document_ids": ["doc_001"],
        "n_tokens": 1200
    },
    {
        "id": "chunk_002_hash",
        "text": "...phÃ¡t triá»ƒn pháº§n má»m AI...",  # tokens 1100-2300 (overlap 100)
        "document_ids": ["doc_001"],
        "n_tokens": 1200
    },
    {
        "id": "chunk_003_hash",
        "text": "...vÃ  má»Ÿ rá»™ng thá»‹ trÆ°á»ng.",  # tokens 2200-2500
        "document_ids": ["doc_001"],
        "n_tokens": 300
    }
]
```

---

### 3. Extract Graph (LLM-based)
**File**: `graphrag/index/workflows/extract_graph.py`

**Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t entities vÃ  relationships tá»« text units báº±ng LLM

**Quy trÃ¬nh**:
```mermaid
graph TB
    A[Text Units] --> B[LLM Graph Extraction]
    B --> C[Extract Entities]
    B --> D[Extract Relationships]
    C --> E[Merge Duplicate Entities]
    D --> F[Merge Duplicate Relationships]
    E --> G[Summarize Descriptions]
    F --> G
    G --> H[Output: entities.parquet]
    G --> I[Output: relationships.parquet]
    
    style A fill:#e8f5e9
    style H fill:#c8e6c9
    style I fill:#c8e6c9
```

**LLM Strategy**:
1. Sá»­ dá»¥ng prompt `extract_graph.txt` Ä‘á»ƒ hÆ°á»›ng dáº«n LLM
2. TrÃ­ch xuáº¥t cÃ¡c entity types Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a (organization, person, geo, event)
3. XÃ¡c Ä‘á»‹nh relationships giá»¯a cÃ¡c entities
4. Gleanings: Láº·p láº¡i extraction Ä‘á»ƒ thu tháº­p thÃªm thÃ´ng tin (max_gleanings)
5. Async processing vá»›i concurrent requests

**Entity Schema**:
- `id`: Entity identifier
- `title`: Entity name
- `type`: Entity type (organization, person, etc.)
- `description`: Summarized description from all mentions
- `text_unit_ids`: List of chunks where entity appears

**Relationship Schema**:
- `id`: Relationship identifier
- `source`: Source entity title
- `target`: Target entity title
- `description`: Relationship description
- `weight`: Relationship strength (optional)
- `text_unit_ids`: List of chunks where relationship appears

**VÃ­ dá»¥ thá»±c táº¿**:
```python
# Input text unit:
"""
CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020 bá»Ÿi Nguyá»…n VÄƒn A á»Ÿ HÃ  Ná»™i. 
CÃ´ng ty chuyÃªn vá» phÃ¡t triá»ƒn pháº§n má»m AI.
"""

# Output entities:
[
    {
        "id": "entity_001",
        "title": "CÃ´ng ty ABC",
        "type": "organization",
        "description": "CÃ´ng ty chuyÃªn vá» phÃ¡t triá»ƒn pháº§n má»m AI, Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020",
        "text_unit_ids": ["chunk_001_hash"]
    },
    {
        "id": "entity_002",
        "title": "Nguyá»…n VÄƒn A",
        "type": "person",
        "description": "NhÃ  sÃ¡ng láº­p CÃ´ng ty ABC",
        "text_unit_ids": ["chunk_001_hash"]
    },
    {
        "id": "entity_003",
        "title": "HÃ  Ná»™i",
        "type": "geo",
        "description": "Äá»‹a Ä‘iá»ƒm thÃ nh láº­p CÃ´ng ty ABC",
        "text_unit_ids": ["chunk_001_hash"]
    }
]

# Output relationships:
[
    {
        "id": "rel_001",
        "source": "Nguyá»…n VÄƒn A",
        "target": "CÃ´ng ty ABC",
        "description": "Nguyá»…n VÄƒn A lÃ  ngÆ°á»i thÃ nh láº­p CÃ´ng ty ABC",
        "weight": 1.0,
        "text_unit_ids": ["chunk_001_hash"]
    },
    {
        "id": "rel_002",
        "source": "CÃ´ng ty ABC",
        "target": "HÃ  Ná»™i",
        "description": "CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p táº¡i HÃ  Ná»™i",
        "weight": 0.8,
        "text_unit_ids": ["chunk_001_hash"]
    }
]
```

---

### 4. Finalize Graph
**File**: `graphrag/index/workflows/finalize_graph.py`

**Má»¥c Ä‘Ã­ch**: LÃ m sáº¡ch vÃ  chuáº©n hÃ³a graph, tÃ­nh toÃ¡n cÃ¡c thuá»™c tÃ­nh bá»• sung

**Quy trÃ¬nh**:
```mermaid
graph TB
    A[Entities + Relationships] --> B{Extraction Type}
    B -->|NLP-based| C[PMI Normalization]
    B -->|LLM-based| D[Sum Weights]
    C --> E[Finalize Entities]
    D --> E
    E --> F[Finalize Relationships]
    F --> G[Compute Node Degree]
    G --> H[Compute Edge Combined Degree]
    H --> I[Remove Duplicates]
    I --> J[Output: Finalized Graph]
    
    style A fill:#e8f5e9
    style J fill:#c8e6c9
```

**Weight Processing khÃ¡c nhau theo extraction method**:

#### NLP-based Extraction (Fast Pipeline)
- **PMI Normalization**: `normalize_edge_weights = True` (default)
- **CÃ´ng thá»©c PMI**: `pmi(x,y) = p(x,y) * log2(p(x,y) / (p(x) * p(y))`
- **Bias correction**: Loáº¡i bá» bias Ä‘á»‘i vá»›i low-frequency events
- **Statistical significance**: TÃ­nh toÃ¡n significance thay vÃ¬ raw counts

#### LLM-based Extraction (Standard Pipeline)
- **No normalization**: KhÃ´ng cÃ³ statistical normalization
- **LLM weights**: Sá»­ dá»¥ng `relationship_strength` scores tá»« LLM
- **Sum aggregation**: `weight = sum(all_occurrence_strengths)`
- **Subjective scoring**: Weights dá»±a trÃªn judgment cá»§a LLM

**Common Processing**:
- Chuáº©n hÃ³a entity names (trim, lowercase for matching)
- TÃ­nh `degree` cho má»—i entity (sá»‘ lÆ°á»£ng relationships)
- TÃ­nh `combined_degree` cho má»—i relationship
- **Final Deduplication**:
  - **Relationships**: `drop_duplicates(subset=["source", "target"])` - giá»¯ láº¡i 1 record duy nháº¥t cho má»—i directed edge
  - **Entities**: `drop_duplicates(subset="title")` - giá»¯ láº¡i 1 record duy nháº¥t cho má»—i entity name
- Remove orphan entities (entities khÃ´ng cÃ³ relationships)
- Generate unique IDs (UUID) vÃ  human_readable_ids

---

### 5. Extract Covariates (Optional)
**File**: `graphrag/index/workflows/extract_covariates.py`

**Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t claims vÃ  facts tá»« text units (náº¿u enabled)

**Output**: `covariates.parquet` chá»©a cÃ¡c claims/facts Ä‘Æ°á»£c trÃ­ch xuáº¥t

---

### 6. Create Communities
**File**: `graphrag/index/workflows/create_communities.py`

**Má»¥c Ä‘Ã­ch**: PhÃ¡t hiá»‡n communities (nhÃ³m entities liÃªn káº¿t cháº·t cháº½) trong graph

**Thuáº­t toÃ¡n**: Hierarchical Leiden Clustering

```mermaid
graph TB
    A[Graph] --> B[Build NetworkX Graph]
    B --> C{Use LCC?}
    C -->|Yes| D[Extract Largest Connected Component]
    C -->|No| E[Use Full Graph]
    D --> F[Hierarchical Leiden]
    E --> F
    F --> G[Multiple Levels of Communities]
    G --> H[Aggregate Entity/Relationship IDs]
    H --> I[Build Community Tree]
    I --> J[Output: communities.parquet]
    
    style A fill:#e8f5e9
    style J fill:#c8e6c9
```

**Parameters**:
- `max_cluster_size`: Maximum size per community (default: 10)
- `use_lcc`: Only use largest connected component
- `seed`: Random seed for reproducibility

**Community Schema**:
- `id`: Community identifier
- `level`: Hierarchy level (0 = lowest, higher = more aggregated)
- `title`: Community title
- `entity_ids`: List of entity IDs in community
- `relationship_ids`: List of relationship IDs within community
- `text_unit_ids`: List of relevant text units
- `parent`: Parent community ID
- `children`: List of child community IDs
- `size`: Number of entities

**VÃ­ dá»¥ thá»±c táº¿**:
```python
# Input: Graph vá»›i 10 entities vÃ  15 relationships
# Communities discovered at 2 levels:

# Level 0 (fine-grained):
[
    {
        "id": "comm_001",
        "level": 0,
        "title": "Community 1",
        "entity_ids": ["entity_001", "entity_002", "entity_003"],
        "relationship_ids": ["rel_001", "rel_002"],
        "parent": 1,  # belongs to community 1 at level 1
        "children": [],
        "size": 3
    },
    {
        "id": "comm_002",
        "level": 0,
        "title": "Community 2",
        "entity_ids": ["entity_004", "entity_005"],
        "relationship_ids": ["rel_003"],
        "parent": 1,
        "children": [],
        "size": 2
    }
]

# Level 1 (coarse-grained):
[
    {
        "id": "comm_003",
        "level": 1,
        "title": "Community 1",
        "entity_ids": ["entity_001", "entity_002", "entity_003", "entity_004", "entity_005"],
        "relationship_ids": ["rel_001", "rel_002", "rel_003"],
        "parent": -1,  # top level
        "children": [1, 2],  # communities 1 and 2 from level 0
        "size": 5
    }
]
```

---

### 7. Create Final Text Units
**File**: `graphrag/index/workflows/create_final_text_units.py`

**Má»¥c Ä‘Ã­ch**: Enrichment text units vá»›i entity vÃ  relationship information

**Output**: Text units Ä‘Æ°á»£c bá»• sung vá»›i:
- List of entity IDs mentioned
- List of relationship IDs present
- Community assignments

---

### 8. Create Community Reports
**File**: `graphrag/index/workflows/create_community_reports.py`

**Má»¥c Ä‘Ã­ch**: Táº¡o summaries cho má»—i community báº±ng LLM

**Quy trÃ¬nh**:
```mermaid
graph TB
    A[Communities + Entities + Relationships] --> B[Build Local Context]
    B --> C[Filter by Token Limit]
    C --> D[LLM Summarization]
    D --> E[Generate Report Fields]
    E --> F[Output: community_reports.parquet]
    
    subgraph "Report Fields"
        G[Title]
        H[Summary]
        I[Full Content]
        J[Findings]
        K[Rating]
    end
    
    E --> G
    E --> H
    E --> I
    E --> J
    E --> K
    
    style A fill:#e8f5e9
    style F fill:#c8e6c9
```

**LLM Strategy**:
1. Collect entities, relationships, vÃ  claims trong community
2. Build local context vá»›i token budget (max_input_length: 8000)
3. Sá»­ dá»¥ng prompts:
   - `community_report_graph.txt` - cho graph-based communities
   - `community_report_text.txt` - cho text-based communities
4. Generate structured report vá»›i cÃ¡c sections

**Report Schema**:
- `id`: Report identifier
- `community`: Community ID
- `level`: Hierarchy level
- `title`: Report title
- `summary`: Executive summary (short)
- `full_content`: Detailed report content
- `findings`: List of key findings (JSON)
- `rating`: Importance rating (float)
- `rating_explanation`: Explanation for rating

**VÃ­ dá»¥ thá»±c táº¿**:
```json
{
    "id": "report_001",
    "community": 1,
    "level": 0,
    "title": "Há»‡ sinh thÃ¡i CÃ´ng ty ABC vÃ  NhÃ  sÃ¡ng láº­p",
    "summary": "Community nÃ y táº­p trung vÃ o CÃ´ng ty ABC, nhÃ  sÃ¡ng láº­p Nguyá»…n VÄƒn A, vÃ  cÃ¡c hoáº¡t Ä‘á»™ng kinh doanh pháº§n má»m AI táº¡i HÃ  Ná»™i.",
    "full_content": "# Tá»•ng quan\n\nCÃ´ng ty ABC lÃ  má»™t tá»• chá»©c...",
    "findings": [
        {
            "summary": "CÃ´ng ty ABC Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020",
            "explanation": "Nguyá»…n VÄƒn A lÃ  ngÆ°á»i sÃ¡ng láº­p cÃ´ng ty nÃ y"
        },
        {
            "summary": "ChuyÃªn mÃ´n vá» AI",
            "explanation": "CÃ´ng ty táº­p trung vÃ o phÃ¡t triá»ƒn pháº§n má»m AI"
        }
    ],
    "rating": 8.5,
    "rating_explanation": "Community cÃ³ táº§m quan trá»ng cao do liÃªn quan Ä‘áº¿n startup cÃ´ng nghá»‡ vÃ  nhÃ¢n váº­t chá»§ chá»‘t"
}
```

---

### 9. Generate Text Embeddings
**File**: `graphrag/index/workflows/generate_text_embeddings.py`

**Má»¥c Ä‘Ã­ch**: Táº¡o vector embeddings cho cÃ¡c text fields Ä‘á»ƒ há»— trá»£ semantic search

**CÃ¡c trÆ°á»ng Ä‘Æ°á»£c embed** (configurable):
1. `document.text` - Document embeddings
2. `entity.title` - Entity name embeddings
3. `entity.description` - Entity description embeddings
4. `relationship.description` - Relationship embeddings
5. `text_unit.text` - Text unit embeddings
6. `community_report.title` - Report title embeddings
7. `community_report.summary` - Report summary embeddings
8. `community_report.full_content` - Full report embeddings

**Quy trÃ¬nh**:
```mermaid
graph TB
    A[Multiple DataFrames] --> B{For each field}
    B --> C[Batch Text]
    C --> D[Embedding Model]
    D --> E[Generate Vectors]
    E --> F[Store in Vector Store]
    F --> G[Output: embeddings.<field>.parquet]
    
    style A fill:#e8f5e9
    style G fill:#c8e6c9
```

**Embedding Strategy**:
- Model: `text-embedding-3-small` (configurable)
- Batch processing vá»›i:
  - `batch_size`: 16 items per batch
  - `batch_max_tokens`: 8191 tokens per batch
- Concurrent requests: 25
- Vector dimension: 1536 (model-dependent)

**Output**: Separate parquet files cho má»—i embedded field
```python
# embeddings.entity.description.parquet
[
    {
        "id": "entity_001",
        "embedding": [0.123, -0.456, 0.789, ...]  # 1536-dim vector
    },
    ...
]
```

**Storage**: Embeddings Ä‘Æ°á»£c lÆ°u vÃ o Vector Store (LanceDB)
- Table per entity type
- Supports similarity search
- Optimized for retrieval

---

## Pipeline Execution Context

### Storage Architecture
```mermaid
graph TB
    A[Input Storage] --> B[Pipeline Run]
    B --> C[Output Storage]
    B --> D[Cache Storage]
    
    C --> E[documents.parquet]
    C --> F[text_units.parquet]
    C --> G[entities.parquet]
    C --> H[relationships.parquet]
    C --> I[communities.parquet]
    C --> J[community_reports.parquet]
    C --> K[embeddings.*]
    
    D --> L[LLM Response Cache]
    D --> M[Embedding Cache]
    
    style A fill:#e8f5e9
    style C fill:#fff9c4
    style D fill:#e1f5fe
```

### Runtime Context
- **Callbacks**: Progress tracking, logging, monitoring
- **Cache**: LLM response caching Ä‘á»ƒ trÃ¡nh duplicate calls
- **Stats**: Performance metrics cho má»—i workflow
- **State**: Shared state between workflows

### Incremental Update Mode

Khi cháº¡y incremental update (`is_update_run=True`):

```mermaid
graph TB
    A[Previous Index] --> B[Backup to previous/]
    C[New Documents] --> D[Run Pipeline]
    D --> E[Delta Index in delta/]
    B --> F[Merge Strategy]
    E --> F
    F --> G[Updated Index]
    
    style A fill:#e8f5e9
    style C fill:#e8f5e9
    style G fill:#c8e6c9
```

**Update Workflows** (cháº¡y sau standard workflows):
1. `update_entities_relationships` - Merge new entities/relationships
2. `update_communities` - Recompute communities
3. `update_community_reports` - Update reports
4. `update_text_embeddings` - Generate embeddings cho new content
5. `update_clean_state` - Cleanup vÃ  finalization

---

## Configuration Example

**File**: `settings.yaml`

```yaml
# Chunking
chunks:
  size: 1200          # tokens per chunk
  overlap: 100        # overlap between chunks
  group_by_columns: [id]

# Entity Extraction (LLM-based)
extract_graph:
  model_id: default_chat_model
  prompt: "prompts/extract_graph.txt"
  entity_types: [organization, person, geo, event]
  max_gleanings: 1    # sá»‘ láº§n iteration Ä‘á»ƒ extract thÃªm

# Entity Extraction (NLP-based - Fast Pipeline)
extract_graph_nlp:
  normalize_edge_weights: true  # PMI normalization cho weights
  text_analyzer:
    model_name: "en_core_web_md"

# Community Detection
cluster_graph:
  max_cluster_size: 10
  use_lcc: true       # chá»‰ dÃ¹ng largest connected component
  seed: 42            # reproducibility

# Community Reports
community_reports:
  model_id: default_chat_model
  max_length: 2000         # max report length
  max_input_length: 8000   # max context for LLM

# Embeddings
embed_text:
  model_id: default_embedding_model
  vector_store_id: default_vector_store
  # Names of fields to embed:
  names:
    - entity.description
    - text_unit.text
    - community_report.summary
```

---

## Performance Considerations

### Bottlenecks
1. **LLM Calls** - Extract graph vÃ  community reports
   - Mitigation: Concurrent requests, caching
2. **Embeddings** - Large number of texts
   - Mitigation: Batching, async processing
3. **Graph Clustering** - Large graphs
   - Mitigation: LCC filtering, max_cluster_size

### Optimization Tips
1. **Use Fast Pipeline** - NLP-based extraction thay vÃ¬ LLM (nhanh hÆ¡n 10-20x)
2. **Weight normalization trade-offs**:
   - NLP-based: Statistical accuracy vá»›i PMI nhÆ°ng cÃ³ thá»ƒ miss complex relationships
   - LLM-based: Semantic richness vá»›i subjective weights nhÆ°ng khÃ´ng statistical
3. **Adjust chunk size** - Balance giá»¯a context vÃ  processing speed
4. **Limit entity types** - Giáº£m complexity cá»§a extraction
5. **Cache aggressively** - Reuse LLM responses
6. **Tune concurrency** - Balance giá»¯a speed vÃ  rate limits

### Monitoring
```python
# stats.json output after indexing
{
    "total_runtime": 1234.56,
    "num_documents": 100,
    "workflows": {
        "load_input_documents": {"overall": 5.2},
        "create_base_text_units": {"overall": 12.8},
        "extract_graph": {"overall": 856.3},
        "create_communities": {"overall": 45.1},
        "create_community_reports": {"overall": 267.4},
        "generate_text_embeddings": {"overall": 47.6}
    }
}
```

---

## VÃ­ dá»¥ thá»±c táº¿ End-to-End

### Input
**File**: `input/company_profile.txt`
```
CÃ´ng ty ABC Technology Ä‘Æ°á»£c thÃ nh láº­p vÃ o thÃ¡ng 3 nÄƒm 2020 bá»Ÿi hai nhÃ  Ä‘á»“ng sÃ¡ng láº­p 
Nguyá»…n VÄƒn A vÃ  Tráº§n Thá»‹ B táº¡i HÃ  Ná»™i. CÃ´ng ty chuyÃªn vá» phÃ¡t triá»ƒn cÃ¡c giáº£i phÃ¡p 
trÃ­ tuá»‡ nhÃ¢n táº¡o cho ngÃ nh tÃ i chÃ­nh.

NÄƒm 2021, cÃ´ng ty Ä‘Ã£ huy Ä‘á»™ng Ä‘Æ°á»£c 5 triá»‡u USD tá»« quá»¹ Ä‘áº§u tÆ° XYZ Ventures. 
CEO Nguyá»…n VÄƒn A cho biáº¿t, nguá»“n vá»‘n nÃ y sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ má»Ÿ rá»™ng Ä‘á»™i ngÅ© 
vÃ  phÃ¡t triá»ƒn sáº£n pháº©m má»›i.

Hiá»‡n táº¡i, ABC Technology cÃ³ vÄƒn phÃ²ng táº¡i HÃ  Ná»™i vÃ  TP.HCM, vá»›i hÆ¡n 50 nhÃ¢n viÃªn.
```

### Processing Steps

**Step 1: Load Documents**
```python
documents = [{
    "id": "doc_001",
    "text": "CÃ´ng ty ABC Technology Ä‘Æ°á»£c thÃ nh láº­p...",
    "metadata": '{"source": "company_profile.txt"}'
}]
```

**Step 2: Create Text Units** (vá»›i chunk_size=200 tokens)
```python
text_units = [
    {
        "id": "chunk_001",
        "text": "CÃ´ng ty ABC Technology Ä‘Æ°á»£c thÃ nh láº­p vÃ o thÃ¡ng 3 nÄƒm 2020...",
        "n_tokens": 200
    },
    {
        "id": "chunk_002", 
        "text": "NÄƒm 2021, cÃ´ng ty Ä‘Ã£ huy Ä‘á»™ng Ä‘Æ°á»£c 5 triá»‡u USD...",
        "n_tokens": 180
    }
]
```

**Step 3: Extract Graph**
```python
entities = [
    {"title": "ABC Technology", "type": "organization"},
    {"title": "Nguyá»…n VÄƒn A", "type": "person"},
    {"title": "Tráº§n Thá»‹ B", "type": "person"},
    {"title": "XYZ Ventures", "type": "organization"},
    {"title": "HÃ  Ná»™i", "type": "geo"},
    {"title": "TP.HCM", "type": "geo"}
]

relationships = [
    {"source": "Nguyá»…n VÄƒn A", "target": "ABC Technology", "description": "Ä‘á»“ng sÃ¡ng láº­p"},
    {"source": "Tráº§n Thá»‹ B", "target": "ABC Technology", "description": "Ä‘á»“ng sÃ¡ng láº­p"},
    {"source": "ABC Technology", "target": "HÃ  Ná»™i", "description": "cÃ³ trá»¥ sá»Ÿ táº¡i"},
    {"source": "XYZ Ventures", "target": "ABC Technology", "description": "Ä‘áº§u tÆ° vÃ o"}
]
```

**Step 4: Create Communities**
```python
communities_level_0 = [
    {
        "id": 1,
        "title": "Community 1",
        "entity_ids": ["ABC Technology", "Nguyá»…n VÄƒn A", "Tráº§n Thá»‹ B"],
        "level": 0
    },
    {
        "id": 2,
        "title": "Community 2",
        "entity_ids": ["XYZ Ventures", "ABC Technology"],
        "level": 0
    }
]
```

**Step 5: Community Reports**
```json
{
    "title": "Há»‡ sinh thÃ¡i ABC Technology",
    "summary": "CÃ´ng ty cÃ´ng nghá»‡ AI cho tÃ i chÃ­nh Ä‘Æ°á»£c thÃ nh láº­p bá»Ÿi 2 founders vÃ  nháº­n Ä‘áº§u tÆ° tá»« XYZ Ventures",
    "findings": [
        "ABC Technology Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2020 táº¡i HÃ  Ná»™i",
        "Nháº­n Ä‘áº§u tÆ° 5 triá»‡u USD nÄƒm 2021",
        "CÃ³ 2 vÄƒn phÃ²ng vÃ  50+ nhÃ¢n viÃªn"
    ],
    "rating": 8.5
}
```

**Step 6: Generate Embeddings**
```python
# Entity embeddings
entity_embeddings = {
    "ABC Technology": [0.123, -0.456, 0.789, ...],  # 1536-dim
    "Nguyá»…n VÄƒn A": [0.234, -0.567, 0.890, ...],
    ...
}

# Report embeddings
report_embeddings = {
    "community_1": [0.345, -0.678, 0.901, ...]
}
```

### Output Structure
```
output/
â”œâ”€â”€ documents.parquet           # 1 document
â”œâ”€â”€ text_units.parquet          # 2 text units
â”œâ”€â”€ entities.parquet            # 6 entities
â”œâ”€â”€ relationships.parquet       # 4 relationships
â”œâ”€â”€ communities.parquet         # 2 communities (level 0)
â”œâ”€â”€ community_reports.parquet   # 2 reports
â”œâ”€â”€ embeddings.entity.description.parquet
â”œâ”€â”€ embeddings.community_report.summary.parquet
â””â”€â”€ stats.json                  # Performance metrics
```

---

## Summary

Pipeline indexing cá»§a GraphRAG chuyá»ƒn Ä‘á»•i raw documents thÃ nh má»™t knowledge graph Ä‘Æ°á»£c cáº¥u trÃºc hÃ³a cao thÃ´ng qua 9 workflows chÃ­nh:

1. **Load Documents** â†’ Parse input
2. **Chunk Text** â†’ Split into processable units
3. **Extract Graph** â†’ LLM extracts entities & relationships
4. **Finalize Graph** â†’ Clean and compute metrics
5. **Extract Covariates** â†’ Extract claims/facts (optional)
6. **Create Communities** â†’ Detect entity clusters
7. **Finalize Text Units** â†’ Enrich with graph data
8. **Create Reports** â†’ LLM summarizes communities
9. **Generate Embeddings** â†’ Vector representations for search

Output lÃ  má»™t rich knowledge graph vá»›i:
- **Entities** vá»›i types vÃ  descriptions
- **Relationships** vá»›i weights
- **Communities** vá»›i hierarchical structure
- **Reports** vá»›i summaries vÃ  findings
- **Embeddings** cho semantic search

Há»‡ thá»‘ng há»— trá»£ cáº£ full indexing vÃ  incremental updates, vá»›i extensive caching vÃ  monitoring capabilities.

---

## ğŸ“¤ Chi Tiáº¿t Output Schema

Pipeline táº¡o ra cÃ¡c báº£ng output dÆ°á»›i dáº¡ng **Parquet files**. Táº¥t cáº£ cÃ¡c báº£ng Ä‘á»u cÃ³ 2 trÆ°á»ng ID chung:

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `id` | str | UUID Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng, Ä‘áº£m báº£o tÃ­nh unique toÃ n cá»¥c |
| `human_readable_id` | int | ID ngáº¯n Ä‘Æ°á»£c increment theo run, dá»… Ä‘á»c cho citations |

---

### ğŸ“ **documents.parquet** - Danh sÃ¡ch Documents

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `title` | str | TÃªn file hoáº·c title Ä‘Æ°á»£c cáº¥u hÃ¬nh |
| `text` | str | Ná»™i dung Ä‘áº§y Ä‘á»§ cá»§a document |
| `text_unit_ids` | str[] | Danh sÃ¡ch text units (chunks) Ä‘Ã£ parse tá»« document |
| `metadata` | dict | Metadata tÃ¹y chá»n náº¿u cáº¥u hÃ¬nh khi import CSV |

---

### ğŸ“ **text_units.parquet** - Danh sÃ¡ch Text Chunks

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `text` | str | Ná»™i dung Ä‘áº§y Ä‘á»§ cá»§a chunk |
| `n_tokens` | int | Sá»‘ tokens trong chunk (thÆ°á»ng = `chunk_size`, trá»« chunk cuá»‘i) |
| `document_ids` | str[] | Danh sÃ¡ch document IDs mÃ  chunk Ä‘Æ°á»£c parse tá»« Ä‘Ã³ |
| `entity_ids` | str[] | Danh sÃ¡ch entities Ä‘Æ°á»£c tÃ¬m tháº¥y trong text unit |
| `relationship_ids` | str[] | Danh sÃ¡ch relationships Ä‘Æ°á»£c tÃ¬m tháº¥y trong text unit |
| `covariate_ids` | str[] | (Optional) Danh sÃ¡ch covariates trong text unit |

---

### ğŸ“ **entities.parquet** - Danh sÃ¡ch Entities

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `title` | str | TÃªn cá»§a entity |
| `type` | str | Loáº¡i entity: "organization", "person", "geo", "event" |
| `description` | str | MÃ´ táº£ cá»§a entity, Ä‘Æ°á»£c LLM tá»•ng há»£p tá»« nhiá»u text units |
| `text_unit_ids` | str[] | Danh sÃ¡ch text units chá»©a entity nÃ y |
| `frequency` | int | Sá»‘ láº§n entity xuáº¥t hiá»‡n trong cÃ¡c text units |
| `degree` | int | Node degree (sá»‘ connections trong graph) |
| `x` | float | Vá»‹ trÃ­ X cho visualization (0 náº¿u khÃ´ng báº­t UMAP) |
| `y` | float | Vá»‹ trÃ­ Y cho visualization (0 náº¿u khÃ´ng báº­t UMAP) |

---

### ğŸ“ **relationships.parquet** - Danh sÃ¡ch Relationships (Edge List)

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `source` | str | TÃªn source entity |
| `target` | str | TÃªn target entity |
| `description` | str | MÃ´ táº£ relationship, Ä‘Æ°á»£c LLM tá»•ng há»£p |
| `weight` | float | Trá»ng sá»‘ edge: **NLP**=PMI-normalized, **LLM**=sum of strengths |
| `combined_degree` | int | Tá»•ng degree cá»§a source vÃ  target nodes |
| `text_unit_ids` | str[] | Danh sÃ¡ch text units chá»©a relationship nÃ y |

---

### ğŸ“ **communities.parquet** - Danh sÃ¡ch Communities (Leiden)

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `community` | int | Leiden community ID (unique qua táº¥t cáº£ levels) |
| `parent` | int | Parent community ID |
| `children` | int[] | Danh sÃ¡ch child community IDs |
| `level` | int | Äá»™ sÃ¢u trong hierarchy (0 = chi tiáº¿t nháº¥t) |
| `title` | str | TÃªn thÃ¢n thiá»‡n cá»§a community |
| `entity_ids` | str[] | Danh sÃ¡ch entity members |
| `relationship_ids` | str[] | Danh sÃ¡ch relationships hoÃ n toÃ n náº±m trong community |
| `text_unit_ids` | str[] | Danh sÃ¡ch text units represented trong community |
| `period` | str | NgÃ y ingest (ISO8601), dÃ¹ng cho incremental updates |
| `size` | int | KÃ­ch thÆ°á»›c community (sá»‘ entities) |

---

### ğŸ“ **community_reports.parquet** - BÃ¡o cÃ¡o Community

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `community` | int | Community ID mÃ  report nÃ y Ã¡p dá»¥ng |
| `parent` | int | Parent community ID |
| `children` | int[] | Danh sÃ¡ch child community IDs |
| `level` | int | Level cá»§a community |
| `title` | str | LLM-generated title cho report |
| `summary` | str | LLM-generated summary |
| `full_content` | str | LLM-generated full report |
| `rank` | float | LLM-derived relevance ranking dá»±a trÃªn entity salience |
| `rating_explanation` | str | LLM-derived giáº£i thÃ­ch vá» rank |
| `findings` | dict | LLM-derived list cá»§a top 5-10 insights (summary + explanation) |
| `full_content_json` | json | Full JSON output tá»« LLM, cho phÃ©p prompt tuning |
| `period` | str | NgÃ y ingest (ISO8601) |
| `size` | int | KÃ­ch thÆ°á»›c community |

---

### ğŸ“ **covariates.parquet** - Claims/Covariates (Optional)

*Chá»‰ Ä‘Æ°á»£c táº¡o khi `extract_claims.enabled = true`*

| TrÆ°á»ng | Type | MÃ´ táº£ |
|--------|------|-------|
| `covariate_type` | str | LuÃ´n lÃ  "claim" vá»›i default config |
| `type` | str | Loáº¡i claim |
| `description` | str | LLM-generated description cá»§a behavior |
| `subject_id` | str | TÃªn source entity (thá»±c hiá»‡n claimed behavior) |
| `object_id` | str | TÃªn target entity (nháº­n claimed behavior) |
| `status` | str | LLM-derived assessment: TRUE, FALSE, hoáº·c SUSPECTED |
| `start_date` | str | LLM-derived ngÃ y báº¯t Ä‘áº§u hÃ nh vi (ISO8601) |
| `end_date` | str | LLM-derived ngÃ y káº¿t thÃºc hÃ nh vi (ISO8601) |
| `source_text` | str | Äoáº¡n text ngáº¯n chá»©a claimed behavior |
| `text_unit_id` | str | ID cá»§a text unit mÃ  claim Ä‘Æ°á»£c extract tá»« Ä‘Ã³ |

---

### ğŸ“ **Cáº¥u trÃºc Output Directory**

```
output/
â”œâ”€â”€ documents.parquet           # TÃ i liá»‡u gá»‘c vá»›i metadata
â”œâ”€â”€ text_units.parquet          # Text chunks vá»›i references
â”œâ”€â”€ entities.parquet            # Entities Ä‘Æ°á»£c trÃ­ch xuáº¥t
â”œâ”€â”€ relationships.parquet       # Relationships giá»¯a entities
â”œâ”€â”€ communities.parquet         # Community assignments
â”œâ”€â”€ community_reports.parquet   # LLM-generated summaries
â”œâ”€â”€ covariates.parquet          # (Optional) Claims/Covariates
â”œâ”€â”€ context.json                # Pipeline state
â”œâ”€â”€ stats.json                  # Execution statistics
â””â”€â”€ embeddings/                 # Vector embeddings (náº¿u enabled)
    â”œâ”€â”€ entity.description.parquet
    â”œâ”€â”€ text_unit.text.parquet
    â””â”€â”€ community_report.summary.parquet
```

---

## Remove Duplicates - Chi tiáº¿t

### Stage 1: Merge trong Extract Graph (Aggregation)
Trong bÆ°á»›c Extract Graph, duplicates Ä‘Ã£ Ä‘Æ°á»£c **aggregated**:

**Entities**:
```python
# groupby(["title", "type"]) vÃ  aggregate
entities.groupby(["title", "type"]).agg({
    description=("description", list),        # Gá»™p táº¥t cáº£ descriptions
    text_unit_ids=("source_id", list),         # Gá»™p táº¥t cáº£ source IDs
    frequency=("source_id", "count")           # Äáº¿m sá»‘ láº§n xuáº¥t hiá»‡n
})
```

**Relationships**:
```python
# groupby(["source", "target"]) vÃ  aggregate  
relationships.groupby(["source", "target"]).agg({
    description=("description", list),          # Gá»™p táº¥t cáº£ descriptions
    text_unit_ids=("source_id", list),         # Gá»™p táº¥t cáº£ source IDs
    weight=("weight", "sum")                    # SUM táº¥t cáº£ weights
})
```

### Stage 2: Final Deduplication (Cleanup)
Trong Finalize Graph, **final cleanup** Ä‘Æ°á»£c thá»±c hiá»‡n:

**Relationships**:
```python
final_relationships = relationships.drop_duplicates(subset=["source", "target"])
# Chá»‰ giá»¯ láº¡i má»™t dÃ²ng cho má»—i (source, target) pair
# ÄÃ£ cÃ³ aggregated weights, descriptions, text_unit_ids tá»« stage 1
```

**Entities**:
```python
final_entities = entities.drop_duplicates(subset="title")
# Chá»‰ giá»¯ láº¡i má»™t dÃ²ng cho má»—i entity title
# ÄÃ£ cÃ³ aggregated info tá»« stage 1
```

### Táº¡i sao cáº§n 2 stages?

#### Standard Pipeline (LLM-based):
1. **Stage 1 (Aggregation)**: Gá»™p thÃ´ng tin tá»« multiple occurrences
2. **Stage 2 (Deduplication)**: **Safety net cho concurrent processing edge cases**

**NguyÃªn nhÃ¢n cáº§n deduplication trong Standard Pipeline:**
```python
# extract_graph.py:63-70 - Concurrent processing
results = await derive_from_rows(
    text_units, run_strategy, async_type=async_mode, num_threads=num_threads
)
# â†’ Multiple text units processed in parallel
# â†’ Rare race conditions or edge cases cÃ³ thá»ƒ create duplicates

# extract_graph.py:76-77 - Collect results from parallel workers
for result in results:
    entity_dfs.append(pd.DataFrame(result[0]))      # Each worker returns DataFrame
    relationship_dfs.append(pd.DataFrame(result[1]))
```

#### Fast Pipeline (NLP-based):
1. **Stage 1 (Aggregation)**: Gá»™p thÃ´ng tin tá»« multiple occurrences
2. **Intermediate Processing**: `prune_graph` vá»›i merge operations cÃ³ thá»ƒ introduce duplicates
3. **Stage 2 (Deduplication)**: Final cleanup sau táº¥t cáº£ transforms

**Root cause duplicates trong Fast Pipeline:**
```python
# prune_graph.py:77-80 - Merge operations
subset_entities = pruned_nodes.merge(entities, on="title", how="inner")
subset_relationships = pruned_edges.merge(relationships, on=["source", "target"], how="inner")
# â†’ Merge cÃ³ thá»ƒ create duplicates náº¿u multiple matches
```

#### Summary:
- **Standard Pipeline**: Concurrent processing edge cases â†’ Need safety net
- **Fast Pipeline**: Prune graph merge operations â†’ Can introduce duplicates  
- **Both pipelines**: `drop_duplicates()` lÃ  **defensive programming** Ä‘áº£m báº£o data integrity

---
